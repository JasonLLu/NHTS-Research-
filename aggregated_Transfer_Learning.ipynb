{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "#tf.enable_eager_execution()\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('encodedStates.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_np(dataframe,shuffle=True,batch_size=32,target=\"HHVEHCNT\"):\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop(target)\n",
    "    return dataframe.values, labels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferLearn:\n",
    "    \n",
    "    def __init__(self, fromState, toState, layers_to_transfer, verbose):\n",
    "        self.data = data\n",
    "        self.fromState = fromState\n",
    "        self.toState = toState\n",
    "        self.layers_to_transfer = layers_to_transfer\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        \n",
    "        self.train_X_from, self.train_y_from = None, None\n",
    "        self.train_X_to, self.train_y_to = None, None\n",
    "        self.fromModelWeights = None\n",
    "    \n",
    "    def initFromStateData(self, sample_size = None):\n",
    "        if sample_size:\n",
    "            self.train_X_from, self.train_y_from = df_to_np(data[self.fromState].sample(sample_size))\n",
    "            return self.train_X_from, self.train_y_from\n",
    "        else:\n",
    "            self.train_X_from, self.train_y_from = df_to_np(data[self.fromState])\n",
    "            return self.train_X_from, self.train_y_from\n",
    "    \n",
    "    def initToStateData(self, sample_size = None):\n",
    "        if sample_size:\n",
    "            self.train_X_to, self.train_y_to = df_to_np(data[self.toState].sample(sample_size))\n",
    "            return self.train_X_to, self.train_y_to\n",
    "        else:\n",
    "            self.train_X_to, self.train_y_to = df_to_np(data[self.toState])\n",
    "            return self.train_X_to, self.train_y_to\n",
    "    \n",
    "    #Get the weights from model built on toStateData\n",
    "    def getFromModelWeights(self, batch_size=100, validation_split=0.2, epochs=20):\n",
    "        train_X_from = self.train_X_from.copy()\n",
    "        train_y_from = self.train_y_from.copy()\n",
    "        model = tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(128,activation=\"relu\"),\n",
    "          tf.keras.layers.Dense(128),\n",
    "          tf.keras.layers.Dense(128),\n",
    "          tf.keras.layers.Dense(128),\n",
    "          tf.keras.layers.Dense(64,activation=\"relu\"),\n",
    "          tf.keras.layers.Dense(4,activation=\"softmax\")\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        model.fit(x=self.train_X_from, y = self.train_y_from,epochs=epochs, batch_size = batch_size, validation_split=validation_split, shuffle=True, verbose=self.verbose)\n",
    "        \n",
    "        weights = []\n",
    "        for i in range(0,self.layers_to_transfer):\n",
    "            weights.append(model.layers[i].get_weights())\n",
    "        self.fromModelWeights = weights\n",
    "        return self.fromModelWeights\n",
    "    \n",
    "    def transfer(self, trainable=True, batch_size=100, validation_split=0.2, epochs=20):\n",
    "        train_X_to = self.train_X_to.copy()\n",
    "        train_y_to = self.train_y_to.copy()\n",
    "        weights = self.fromModelWeights if self.fromModelWeights else self.getFromModelWeights()\n",
    "        print(\"*****weights obtained from fromStateData*****\")\n",
    "        \n",
    "        print(\"Transferring\")\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(128,activation=\"relu\", weights=weights[0], trainable=trainable))\n",
    "        for i in range(1,self.layers_to_transfer):\n",
    "            model.add(tf.keras.layers.Dense(128, weights=weights[i], trainable=trainable))\n",
    "        for i in range(4-self.layers_to_transfer,4):\n",
    "            model.add(tf.keras.layers.Dense(128))\n",
    "        model.add(tf.keras.layers.Dense(64,activation=\"relu\"))\n",
    "        model.add(tf.keras.layers.Dense(4,activation=\"softmax\"))\n",
    "\n",
    "        \n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        history = model.fit(x=self.train_X_to, y = self.train_y_to, epochs=epochs, batch_size = batch_size, validation_split=validation_split, shuffle=True,verbose=self.verbose)\n",
    "        val_acc = history.history[\"val_accuracy\"][-1]\n",
    "        print(\"*****Transferring done*****\", \"Trainable:\", trainable)\n",
    "        return model, val_acc\n",
    "    \n",
    "    \n",
    "    def benchmark(self, batch_size=100, validation_split=0.2, epochs=20):\n",
    "        train_X_to = self.train_X_to.copy()\n",
    "        train_y_to = self.train_y_to.copy()\n",
    "        print(\"*****Training Benchmark Model*****\")\n",
    "        model = tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(128,activation=\"relu\"),\n",
    "          tf.keras.layers.Dense(128),\n",
    "          tf.keras.layers.Dense(128),\n",
    "          tf.keras.layers.Dense(128),\n",
    "          tf.keras.layers.Dense(64,activation=\"relu\"),\n",
    "          tf.keras.layers.Dense(4,activation=\"softmax\")\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        history = model.fit(x=self.train_X_to, y = self.train_y_to,epochs=epochs, batch_size = batch_size, validation_split=validation_split, shuffle=True,verbose=self.verbose)\n",
    "        val_acc = history.history[\"val_accuracy\"][-1]\n",
    "        print(\"*****Training Benchmark Model Done*****\")\n",
    "        return model, val_acc\n",
    "    \n",
    "    def compare(self):\n",
    "        benchmark_model, benchmark_acc = self.benchmark()\n",
    "        frozenTransfer_model, frozenTransfer_acc = self.transfer(trainable=False)\n",
    "        unfrozenTransfer_model, unfrozenTransfer_acc = self.transfer(trainable=True)\n",
    "        \n",
    "        return (benchmark_acc-frozenTransfer_acc, benchmark_acc-unfrozenTransfer_acc)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Training Benchmark Model*****\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/15\n",
      "400/400 [==============================] - 1s 2ms/sample - loss: 1.2770 - accuracy: 0.3425 - val_loss: 1.1513 - val_accuracy: 0.4500\n",
      "Epoch 2/15\n",
      "400/400 [==============================] - 0s 192us/sample - loss: 1.0977 - accuracy: 0.4900 - val_loss: 1.1052 - val_accuracy: 0.4100\n",
      "Epoch 3/15\n",
      "400/400 [==============================] - 0s 158us/sample - loss: 1.0424 - accuracy: 0.5000 - val_loss: 1.0691 - val_accuracy: 0.4500\n",
      "Epoch 4/15\n",
      "400/400 [==============================] - 0s 89us/sample - loss: 0.9863 - accuracy: 0.5575 - val_loss: 1.0402 - val_accuracy: 0.5100\n",
      "Epoch 5/15\n",
      "400/400 [==============================] - 0s 133us/sample - loss: 0.9533 - accuracy: 0.5700 - val_loss: 1.0342 - val_accuracy: 0.5100\n",
      "Epoch 6/15\n",
      "400/400 [==============================] - 0s 101us/sample - loss: 0.9368 - accuracy: 0.5825 - val_loss: 1.0034 - val_accuracy: 0.5700\n",
      "Epoch 7/15\n",
      "400/400 [==============================] - 0s 131us/sample - loss: 0.9182 - accuracy: 0.5900 - val_loss: 0.9916 - val_accuracy: 0.5600\n",
      "Epoch 8/15\n",
      "400/400 [==============================] - 0s 123us/sample - loss: 0.8957 - accuracy: 0.6050 - val_loss: 0.9883 - val_accuracy: 0.5800\n",
      "Epoch 9/15\n",
      "400/400 [==============================] - 0s 119us/sample - loss: 0.9058 - accuracy: 0.5800 - val_loss: 0.9956 - val_accuracy: 0.5500\n",
      "Epoch 10/15\n",
      "400/400 [==============================] - 0s 149us/sample - loss: 0.8759 - accuracy: 0.6100 - val_loss: 0.9909 - val_accuracy: 0.6200\n",
      "Epoch 11/15\n",
      "400/400 [==============================] - 0s 102us/sample - loss: 0.8694 - accuracy: 0.6100 - val_loss: 0.9848 - val_accuracy: 0.5600\n",
      "Epoch 12/15\n",
      "400/400 [==============================] - 0s 92us/sample - loss: 0.8560 - accuracy: 0.5925 - val_loss: 0.9750 - val_accuracy: 0.5900\n",
      "Epoch 13/15\n",
      "400/400 [==============================] - 0s 98us/sample - loss: 0.8454 - accuracy: 0.6050 - val_loss: 0.9756 - val_accuracy: 0.5800\n",
      "Epoch 14/15\n",
      "400/400 [==============================] - 0s 99us/sample - loss: 0.8378 - accuracy: 0.6200 - val_loss: 0.9829 - val_accuracy: 0.5800\n",
      "Epoch 15/15\n",
      "400/400 [==============================] - 0s 72us/sample - loss: 0.8484 - accuracy: 0.6025 - val_loss: 0.9756 - val_accuracy: 0.5800\n",
      "*****Training Benchmark Model Done*****\n",
      "Train on 2568 samples, validate on 643 samples\n",
      "Epoch 1/15\n",
      "2568/2568 [==============================] - 1s 428us/sample - loss: 1.0563 - accuracy: 0.5635 - val_loss: 1.0388 - val_accuracy: 0.5459\n",
      "Epoch 2/15\n",
      "2568/2568 [==============================] - 0s 48us/sample - loss: 0.8597 - accuracy: 0.6729 - val_loss: 1.0476 - val_accuracy: 0.5552\n",
      "Epoch 3/15\n",
      "2568/2568 [==============================] - 0s 42us/sample - loss: 0.8051 - accuracy: 0.6850 - val_loss: 1.0902 - val_accuracy: 0.5754\n",
      "Epoch 4/15\n",
      "2568/2568 [==============================] - 0s 50us/sample - loss: 0.7858 - accuracy: 0.6893 - val_loss: 1.1341 - val_accuracy: 0.5677\n",
      "Epoch 5/15\n",
      "2568/2568 [==============================] - 0s 146us/sample - loss: 0.7480 - accuracy: 0.6994 - val_loss: 1.1303 - val_accuracy: 0.5599\n",
      "Epoch 6/15\n",
      "2568/2568 [==============================] - 0s 93us/sample - loss: 0.7258 - accuracy: 0.7029 - val_loss: 1.2095 - val_accuracy: 0.5148\n",
      "Epoch 7/15\n",
      "2568/2568 [==============================] - 0s 85us/sample - loss: 0.7287 - accuracy: 0.6959 - val_loss: 1.2425 - val_accuracy: 0.5521\n",
      "Epoch 8/15\n",
      "2568/2568 [==============================] - 0s 42us/sample - loss: 0.7024 - accuracy: 0.7138 - val_loss: 1.2463 - val_accuracy: 0.5163\n",
      "Epoch 9/15\n",
      "2568/2568 [==============================] - 0s 44us/sample - loss: 0.6907 - accuracy: 0.7118 - val_loss: 1.4472 - val_accuracy: 0.4837\n",
      "Epoch 10/15\n",
      "2568/2568 [==============================] - 0s 130us/sample - loss: 0.7002 - accuracy: 0.7103 - val_loss: 1.2420 - val_accuracy: 0.5583\n",
      "Epoch 11/15\n",
      "2568/2568 [==============================] - 0s 107us/sample - loss: 0.6856 - accuracy: 0.7165 - val_loss: 1.4475 - val_accuracy: 0.5117\n",
      "Epoch 12/15\n",
      "2568/2568 [==============================] - 0s 136us/sample - loss: 0.6864 - accuracy: 0.7208 - val_loss: 1.3269 - val_accuracy: 0.5381\n",
      "Epoch 13/15\n",
      "2568/2568 [==============================] - 0s 114us/sample - loss: 0.6615 - accuracy: 0.7247 - val_loss: 1.3306 - val_accuracy: 0.5397\n",
      "Epoch 14/15\n",
      "2568/2568 [==============================] - 0s 48us/sample - loss: 0.6600 - accuracy: 0.7255 - val_loss: 1.3901 - val_accuracy: 0.5179\n",
      "Epoch 15/15\n",
      "2568/2568 [==============================] - 0s 60us/sample - loss: 0.6533 - accuracy: 0.7192 - val_loss: 1.3651 - val_accuracy: 0.5179\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/15\n",
      "400/400 [==============================] - 1s 2ms/sample - loss: 1.2754 - accuracy: 0.3950 - val_loss: 1.1664 - val_accuracy: 0.5100\n",
      "Epoch 2/15\n",
      "400/400 [==============================] - 0s 90us/sample - loss: 1.0935 - accuracy: 0.5600 - val_loss: 1.0907 - val_accuracy: 0.5100\n",
      "Epoch 3/15\n",
      "400/400 [==============================] - 0s 81us/sample - loss: 1.0132 - accuracy: 0.5825 - val_loss: 1.0401 - val_accuracy: 0.5100\n",
      "Epoch 4/15\n",
      "400/400 [==============================] - 0s 85us/sample - loss: 0.9719 - accuracy: 0.5750 - val_loss: 1.0107 - val_accuracy: 0.5100\n",
      "Epoch 5/15\n",
      "400/400 [==============================] - 0s 82us/sample - loss: 0.9539 - accuracy: 0.5775 - val_loss: 1.0113 - val_accuracy: 0.5000\n",
      "Epoch 6/15\n",
      "400/400 [==============================] - 0s 119us/sample - loss: 0.9366 - accuracy: 0.6050 - val_loss: 1.0153 - val_accuracy: 0.5200\n",
      "Epoch 7/15\n",
      "400/400 [==============================] - 0s 101us/sample - loss: 0.9281 - accuracy: 0.5850 - val_loss: 1.0118 - val_accuracy: 0.5400\n",
      "Epoch 8/15\n",
      "400/400 [==============================] - 0s 140us/sample - loss: 0.9142 - accuracy: 0.6125 - val_loss: 0.9978 - val_accuracy: 0.5600\n",
      "Epoch 9/15\n",
      "400/400 [==============================] - 0s 102us/sample - loss: 0.8992 - accuracy: 0.6125 - val_loss: 0.9961 - val_accuracy: 0.5400\n",
      "Epoch 10/15\n",
      "400/400 [==============================] - 0s 67us/sample - loss: 0.8900 - accuracy: 0.6175 - val_loss: 1.0020 - val_accuracy: 0.5400\n",
      "Epoch 11/15\n",
      "400/400 [==============================] - 0s 60us/sample - loss: 0.8844 - accuracy: 0.6250 - val_loss: 1.0098 - val_accuracy: 0.5300\n",
      "Epoch 12/15\n",
      "400/400 [==============================] - 0s 66us/sample - loss: 0.8740 - accuracy: 0.6200 - val_loss: 1.0154 - val_accuracy: 0.5500\n",
      "Epoch 13/15\n",
      "400/400 [==============================] - 0s 63us/sample - loss: 0.8679 - accuracy: 0.6250 - val_loss: 0.9994 - val_accuracy: 0.5400\n",
      "Epoch 14/15\n",
      "400/400 [==============================] - 0s 63us/sample - loss: 0.8591 - accuracy: 0.6275 - val_loss: 1.0004 - val_accuracy: 0.5500\n",
      "Epoch 15/15\n",
      "400/400 [==============================] - 0s 64us/sample - loss: 0.8532 - accuracy: 0.6275 - val_loss: 1.0115 - val_accuracy: 0.5500\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/15\n",
      "400/400 [==============================] - 1s 3ms/sample - loss: 1.2324 - accuracy: 0.3875 - val_loss: 1.1232 - val_accuracy: 0.4400\n",
      "Epoch 2/15\n",
      "400/400 [==============================] - 0s 116us/sample - loss: 1.0620 - accuracy: 0.5050 - val_loss: 1.0379 - val_accuracy: 0.5300\n",
      "Epoch 3/15\n",
      "400/400 [==============================] - 0s 86us/sample - loss: 0.9956 - accuracy: 0.5575 - val_loss: 0.9839 - val_accuracy: 0.5400\n",
      "Epoch 4/15\n",
      "400/400 [==============================] - 0s 97us/sample - loss: 0.9486 - accuracy: 0.5875 - val_loss: 1.0024 - val_accuracy: 0.5500\n",
      "Epoch 5/15\n",
      "400/400 [==============================] - 0s 64us/sample - loss: 0.9231 - accuracy: 0.5950 - val_loss: 1.0172 - val_accuracy: 0.5600\n",
      "Epoch 6/15\n",
      "400/400 [==============================] - 0s 94us/sample - loss: 0.8929 - accuracy: 0.6100 - val_loss: 0.9884 - val_accuracy: 0.5900\n",
      "Epoch 7/15\n",
      "400/400 [==============================] - 0s 83us/sample - loss: 0.8761 - accuracy: 0.6250 - val_loss: 0.9930 - val_accuracy: 0.5600\n",
      "Epoch 8/15\n",
      "400/400 [==============================] - 0s 125us/sample - loss: 0.8630 - accuracy: 0.6250 - val_loss: 0.9908 - val_accuracy: 0.5900\n",
      "Epoch 9/15\n",
      "400/400 [==============================] - 0s 171us/sample - loss: 0.8504 - accuracy: 0.6325 - val_loss: 0.9952 - val_accuracy: 0.5800\n",
      "Epoch 10/15\n",
      "400/400 [==============================] - 0s 198us/sample - loss: 0.8457 - accuracy: 0.6300 - val_loss: 0.9984 - val_accuracy: 0.5900\n",
      "Epoch 11/15\n",
      "400/400 [==============================] - 0s 142us/sample - loss: 0.8376 - accuracy: 0.6525 - val_loss: 1.0228 - val_accuracy: 0.5600\n",
      "Epoch 12/15\n",
      "400/400 [==============================] - 0s 102us/sample - loss: 0.8282 - accuracy: 0.6425 - val_loss: 0.9985 - val_accuracy: 0.5900\n",
      "Epoch 13/15\n",
      "400/400 [==============================] - 0s 115us/sample - loss: 0.8267 - accuracy: 0.6350 - val_loss: 0.9959 - val_accuracy: 0.6000\n",
      "Epoch 14/15\n",
      "400/400 [==============================] - 0s 84us/sample - loss: 0.8198 - accuracy: 0.6425 - val_loss: 1.0096 - val_accuracy: 0.5600\n",
      "Epoch 15/15\n",
      "400/400 [==============================] - 0s 96us/sample - loss: 0.8169 - accuracy: 0.6425 - val_loss: 1.0017 - val_accuracy: 0.5800\n",
      "*****Transferring done***** Trainable: True\n"
     ]
    }
   ],
   "source": [
    "MA_to_NY = TransferLearn(fromState = \"MA\", toState = \"NY\", layers_to_transfer=2, verbose = 1)\n",
    "MA_to_NY.initFromStateData()\n",
    "MA_to_NY.initToStateData(500)\n",
    "compareMAandNY = MA_to_NY.compare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.029999971, 0.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compareMAandNY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['NC', 'WI', 'NY', 'MD', 'PA', 'CA', 'TX', 'AZ', 'WA', 'IL', 'KY', 'MT', 'IA', 'GA', 'ME', 'VA', 'SC', 'WV', 'FL', 'NH', 'MN', 'NE', 'AR', 'NJ', 'SD', 'NM', 'OK', 'MI', 'VT', 'ID', 'DE', 'MA', 'WY', 'CO', 'IN', 'AL', 'TN', 'HI', 'AK', 'OH', 'RI', 'LA', 'OR', 'KS', 'UT', 'MO', 'DC', 'NV', 'ND', 'MS', 'CT'])\n"
     ]
    }
   ],
   "source": [
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [\"MA\", \"NY\", \"AK\", \"CO\"]\n",
    "I,J = len(states), len(states)\n",
    "similarity_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing MA MA layers transferred: 1\n",
      "Processing MA CA layers transferred: 1\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing MA NY layers transferred: 1\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing CA MA layers transferred: 1\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing CA CA layers transferred: 1\n",
      "Processing CA NY layers transferred: 1\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing NY MA layers transferred: 1\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing NY CA layers transferred: 1\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing NY NY layers transferred: 1\n",
      "[[[ 0.          0.00743616 -0.00063652]\n",
      "  [ 0.01244169  0.         -0.0251677 ]\n",
      "  [-0.01088649  0.00227571  0.        ]]\n",
      "\n",
      " [[ 0.          0.00278854  0.00088137]\n",
      "  [ 0.01244169  0.         -0.02536356]\n",
      "  [-0.01088649 -0.00192314  0.        ]]]\n",
      "Processing MA MA layers transferred: 2\n",
      "Processing MA CA layers transferred: 2\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing MA NY layers transferred: 2\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing CA MA layers transferred: 2\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing CA CA layers transferred: 2\n",
      "Processing CA NY layers transferred: 2\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing NY MA layers transferred: 2\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing NY CA layers transferred: 2\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing NY NY layers transferred: 2\n",
      "[[[ 0.          0.0048399   0.00665915]\n",
      "  [-0.0279938   0.         -0.00225234]\n",
      "  [ 0.02021772 -0.01631463  0.        ]]\n",
      "\n",
      " [[ 0.          0.00608993  0.00014687]\n",
      "  [ 0.01866251  0.          0.00053859]\n",
      "  [ 0.0155521  -0.00842977  0.        ]]]\n",
      "Processing MA MA layers transferred: 3\n",
      "Processing MA CA layers transferred: 3\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing MA NY layers transferred: 3\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing CA MA layers transferred: 3\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing CA CA layers transferred: 3\n",
      "Processing CA NY layers transferred: 3\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing NY MA layers transferred: 3\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing NY CA layers transferred: 3\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing NY NY layers transferred: 3\n",
      "[[[ 0.         -0.00522453  0.0012241 ]\n",
      "  [ 0.06220835  0.          0.00254619]\n",
      "  [-0.00622082 -0.00480783  0.        ]]\n",
      "\n",
      " [[ 0.          0.00012821  0.00014687]\n",
      "  [ 0.10575426  0.         -0.00509226]\n",
      "  [-0.00933129 -0.01230806  0.        ]]]\n",
      "Processing MA MA layers transferred: 4\n",
      "Processing MA CA layers transferred: 4\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing MA NY layers transferred: 4\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing CA MA layers transferred: 4\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing CA CA layers transferred: 4\n",
      "Processing CA NY layers transferred: 4\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing NY MA layers transferred: 4\n",
      "*****Training Benchmark Model*****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing NY CA layers transferred: 4\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing NY NY layers transferred: 4\n",
      "[[[ 0.00000000e+00 -8.01265240e-04  4.60267067e-03]\n",
      "  [-6.53188229e-02  0.00000000e+00 -3.08471918e-03]\n",
      "  [-1.86625123e-02  9.93609428e-04  0.00000000e+00]]\n",
      "\n",
      " [[ 0.00000000e+00  3.23730707e-03 -8.03011656e-03]\n",
      "  [-2.48833895e-02  0.00000000e+00 -2.00748444e-03]\n",
      "  [ 1.71073079e-02  9.61422920e-05  0.00000000e+00]]]\n"
     ]
    }
   ],
   "source": [
    "layers_to_transfer = 4\n",
    "similarity_list.append(np.zeros((2,I,J)))\n",
    "#We can only transfer at most 4 layers for now\n",
    "for n in range(1,layers_to_transfer + 1):\n",
    "    #n of IxJ matrices\n",
    "    similarity = np.zeros((2,I,J))\n",
    "    for r in range(I):\n",
    "        fromState = states[r]\n",
    "        for c in range(J):\n",
    "            toState = states[c]\n",
    "            print(\"Processing\", fromState, toState, \"layers transferred:\", n)\n",
    "            if fromState != toState:\n",
    "                transferLearner = TransferLearn(fromStateData = data[fromState], toStateData = data[toState], layers_to_transfer=n,verbose=0)\n",
    "                frozen_diff, unfrozen_diff = transferLearner.compare()\n",
    "            else:\n",
    "                frozen_diff, unfrozen_diff = 0, 0\n",
    "            similarity[0][r][c] = frozen_diff\n",
    "            similarity[1][r][c] = unfrozen_diff\n",
    "    print(similarity)\n",
    "    similarity_list.append(similarity)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred 0 layers:\n",
      "[[[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]]\n",
      "Transferred 1 layers:\n",
      "[[[ 0.          0.00743616 -0.00063652]\n",
      "  [ 0.01244169  0.         -0.0251677 ]\n",
      "  [-0.01088649  0.00227571  0.        ]]\n",
      "\n",
      " [[ 0.          0.00278854  0.00088137]\n",
      "  [ 0.01244169  0.         -0.02536356]\n",
      "  [-0.01088649 -0.00192314  0.        ]]]\n",
      "Transferred 2 layers:\n",
      "[[[ 0.          0.0048399   0.00665915]\n",
      "  [-0.0279938   0.         -0.00225234]\n",
      "  [ 0.02021772 -0.01631463  0.        ]]\n",
      "\n",
      " [[ 0.          0.00608993  0.00014687]\n",
      "  [ 0.01866251  0.          0.00053859]\n",
      "  [ 0.0155521  -0.00842977  0.        ]]]\n",
      "Transferred 3 layers:\n",
      "[[[ 0.         -0.00522453  0.0012241 ]\n",
      "  [ 0.06220835  0.          0.00254619]\n",
      "  [-0.00622082 -0.00480783  0.        ]]\n",
      "\n",
      " [[ 0.          0.00012821  0.00014687]\n",
      "  [ 0.10575426  0.         -0.00509226]\n",
      "  [-0.00933129 -0.01230806  0.        ]]]\n",
      "Transferred 4 layers:\n",
      "[[[ 0.00000000e+00 -8.01265240e-04  4.60267067e-03]\n",
      "  [-6.53188229e-02  0.00000000e+00 -3.08471918e-03]\n",
      "  [-1.86625123e-02  9.93609428e-04  0.00000000e+00]]\n",
      "\n",
      " [[ 0.00000000e+00  3.23730707e-03 -8.03011656e-03]\n",
      "  [-2.48833895e-02  0.00000000e+00 -2.00748444e-03]\n",
      "  [ 1.71073079e-02  9.61422920e-05  0.00000000e+00]]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(similarity_list)):\n",
    "    print(\"Transferred\",i,\"layers:\")\n",
    "    print(similarity_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing MA MA layers transferred: 1\n",
      "Processing MA NY layers transferred: 1\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing MA AK layers transferred: 1\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing MA CO layers transferred: 1\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing NY MA layers transferred: 1\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing NY NY layers transferred: 1\n",
      "Processing NY AK layers transferred: 1\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing NY CO layers transferred: 1\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing AK MA layers transferred: 1\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing AK NY layers transferred: 1\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing AK AK layers transferred: 1\n",
      "Processing AK CO layers transferred: 1\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing CO MA layers transferred: 1\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing CO NY layers transferred: 1\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing CO AK layers transferred: 1\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing CO CO layers transferred: 1\n",
      "[[[ 0.          0.04999998 -0.01999998  0.00999999]\n",
      "  [-0.04999995  0.         -0.02000004 -0.05000001]\n",
      "  [ 0.00999999 -0.04000002  0.         -0.01999998]\n",
      "  [ 0.02999997  0.         -0.02999997  0.        ]]\n",
      "\n",
      " [[ 0.          0.04999998 -0.05000001  0.01999998]\n",
      "  [-0.00999999  0.         -0.03000003 -0.02999997]\n",
      "  [-0.01999998 -0.01999998  0.         -0.05000001]\n",
      "  [ 0.01999998  0.00999999 -0.01999998  0.        ]]]\n",
      "Processing MA MA layers transferred: 2\n",
      "Processing MA NY layers transferred: 2\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing MA AK layers transferred: 2\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing MA CO layers transferred: 2\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing NY MA layers transferred: 2\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing NY NY layers transferred: 2\n",
      "Processing NY AK layers transferred: 2\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing NY CO layers transferred: 2\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing AK MA layers transferred: 2\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing AK NY layers transferred: 2\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing AK AK layers transferred: 2\n",
      "Processing AK CO layers transferred: 2\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing CO MA layers transferred: 2\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing CO NY layers transferred: 2\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing CO AK layers transferred: 2\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing CO CO layers transferred: 2\n",
      "[[[ 0.         -0.01999998 -0.00999999  0.02000004]\n",
      "  [-0.01999998  0.         -0.02000004 -0.06      ]\n",
      "  [ 0.06999999  0.03000003  0.          0.        ]\n",
      "  [ 0.06       -0.00999999 -0.22000003  0.        ]]\n",
      "\n",
      " [[ 0.         -0.03000003 -0.00999999  0.        ]\n",
      "  [-0.00999999  0.         -0.02000004 -0.08999997]\n",
      "  [ 0.04000002  0.03000003  0.         -0.00999999]\n",
      "  [ 0.04000002 -0.00999999 -0.13999999  0.        ]]]\n",
      "Processing MA MA layers transferred: 3\n",
      "Processing MA NY layers transferred: 3\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing MA AK layers transferred: 3\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing MA CO layers transferred: 3\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing NY MA layers transferred: 3\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing NY NY layers transferred: 3\n",
      "Processing NY AK layers transferred: 3\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing NY CO layers transferred: 3\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing AK MA layers transferred: 3\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing AK NY layers transferred: 3\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing AK AK layers transferred: 3\n",
      "Processing AK CO layers transferred: 3\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing CO MA layers transferred: 3\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing CO NY layers transferred: 3\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing CO AK layers transferred: 3\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing CO CO layers transferred: 3\n",
      "[[[ 0.          0.01999998 -0.00999999 -0.10000002]\n",
      "  [-0.00999999  0.          0.         -0.00999999]\n",
      "  [ 0.          0.          0.          0.00999999]\n",
      "  [-0.01999998  0.         -0.04000002  0.        ]]\n",
      "\n",
      " [[ 0.          0.03000003 -0.03000003 -0.03000003]\n",
      "  [-0.01999998  0.         -0.00999999 -0.00999999]\n",
      "  [ 0.         -0.04000002  0.          0.        ]\n",
      "  [-0.01999998 -0.01999998 -0.12        0.        ]]]\n",
      "Processing MA MA layers transferred: 4\n",
      "Processing MA NY layers transferred: 4\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing MA AK layers transferred: 4\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing MA CO layers transferred: 4\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing NY MA layers transferred: 4\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing NY NY layers transferred: 4\n",
      "Processing NY AK layers transferred: 4\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing NY CO layers transferred: 4\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing AK MA layers transferred: 4\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing AK NY layers transferred: 4\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing AK AK layers transferred: 4\n",
      "Processing AK CO layers transferred: 4\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing CO MA layers transferred: 4\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing CO NY layers transferred: 4\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing CO AK layers transferred: 4\n",
      "*****Training Benchmark Model*****\n",
      "*****Training Benchmark Model Done*****\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: False\n",
      "*****weights obtained from fromStateData*****\n",
      "Transferring\n",
      "*****Transferring done***** Trainable: True\n",
      "Processing CO CO layers transferred: 4\n",
      "[[[ 0.         -0.00999999  0.01999998  0.02999997]\n",
      "  [-0.04000002  0.          0.01999998 -0.06999999]\n",
      "  [ 0.00999999  0.          0.         -0.01999998]\n",
      "  [ 0.00999999 -0.06        0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.04999998 -0.01999998 -0.01000005]\n",
      "  [-0.03000003  0.         -0.06       -0.10999995]\n",
      "  [ 0.00999999  0.00999999  0.         -0.02999997]\n",
      "  [ 0.04000002 -0.02999997  0.00999999  0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "layers_to_transfer = 4\n",
    "similarity_list.append(np.zeros((2,I,J)))\n",
    "#We can only transfer at most 4 layers for now\n",
    "for n in range(1,layers_to_transfer + 1):\n",
    "    #n of IxJ matrices\n",
    "    similarity = np.zeros((2,I,J))\n",
    "    for r in range(I):\n",
    "        fromState = states[r]\n",
    "        transferLearner = TransferLearn(fromState = fromState, toState = None, layers_to_transfer=n,verbose=0)\n",
    "        transferLearner.initFromStateData()\n",
    "        transferLearner.getFromModelWeights()\n",
    "        for c in range(J):\n",
    "            toState = states[c]\n",
    "            print(\"Processing\", fromState, toState, \"layers transferred:\", n)\n",
    "            if fromState != toState:\n",
    "                transferLearner.toState = toState\n",
    "                transferLearner.initToStateData(sample_size=500)\n",
    "                frozen_diff, unfrozen_diff = transferLearner.compare()\n",
    "            else:\n",
    "                frozen_diff, unfrozen_diff = 0, 0\n",
    "            similarity[0][r][c] = frozen_diff\n",
    "            similarity[1][r][c] = unfrozen_diff\n",
    "    print(similarity)\n",
    "    similarity_list.append(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred 0 layers:\n",
      "[[[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]]\n",
      "Transferred 1 layers:\n",
      "[[[ 0.          0.04999998 -0.01999998  0.00999999]\n",
      "  [-0.04999995  0.         -0.02000004 -0.05000001]\n",
      "  [ 0.00999999 -0.04000002  0.         -0.01999998]\n",
      "  [ 0.02999997  0.         -0.02999997  0.        ]]\n",
      "\n",
      " [[ 0.          0.04999998 -0.05000001  0.01999998]\n",
      "  [-0.00999999  0.         -0.03000003 -0.02999997]\n",
      "  [-0.01999998 -0.01999998  0.         -0.05000001]\n",
      "  [ 0.01999998  0.00999999 -0.01999998  0.        ]]]\n",
      "Transferred 2 layers:\n",
      "[[[ 0.         -0.01999998 -0.00999999  0.02000004]\n",
      "  [-0.01999998  0.         -0.02000004 -0.06      ]\n",
      "  [ 0.06999999  0.03000003  0.          0.        ]\n",
      "  [ 0.06       -0.00999999 -0.22000003  0.        ]]\n",
      "\n",
      " [[ 0.         -0.03000003 -0.00999999  0.        ]\n",
      "  [-0.00999999  0.         -0.02000004 -0.08999997]\n",
      "  [ 0.04000002  0.03000003  0.         -0.00999999]\n",
      "  [ 0.04000002 -0.00999999 -0.13999999  0.        ]]]\n",
      "Transferred 3 layers:\n",
      "[[[ 0.          0.01999998 -0.00999999 -0.10000002]\n",
      "  [-0.00999999  0.          0.         -0.00999999]\n",
      "  [ 0.          0.          0.          0.00999999]\n",
      "  [-0.01999998  0.         -0.04000002  0.        ]]\n",
      "\n",
      " [[ 0.          0.03000003 -0.03000003 -0.03000003]\n",
      "  [-0.01999998  0.         -0.00999999 -0.00999999]\n",
      "  [ 0.         -0.04000002  0.          0.        ]\n",
      "  [-0.01999998 -0.01999998 -0.12        0.        ]]]\n",
      "Transferred 4 layers:\n",
      "[[[ 0.         -0.00999999  0.01999998  0.02999997]\n",
      "  [-0.04000002  0.          0.01999998 -0.06999999]\n",
      "  [ 0.00999999  0.          0.         -0.01999998]\n",
      "  [ 0.00999999 -0.06        0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.04999998 -0.01999998 -0.01000005]\n",
      "  [-0.03000003  0.         -0.06       -0.10999995]\n",
      "  [ 0.00999999  0.00999999  0.         -0.02999997]\n",
      "  [ 0.04000002 -0.02999997  0.00999999  0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "#run 1\n",
    "for i in range(len(similarity_list)):\n",
    "    print(\"Transferred\",i,\"layers:\")\n",
    "    print(similarity_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
